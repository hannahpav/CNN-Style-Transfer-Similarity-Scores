{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-07-24T01:59:36.470001Z",
     "start_time": "2024-07-24T01:59:35.187672Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.utils import save_image\n",
    "import pandas as pd\n",
    "\n",
    "import ssl\n",
    "import certifi\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "306617d55ec8f23c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T01:59:37.075406Z",
     "start_time": "2024-07-24T01:59:37.072857Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device = mps\n"
     ]
    }
   ],
   "source": [
    "device = 'mps' if torch.backends.mps.is_available() else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device = \" + device)\n",
    "if device == 'cpu':\n",
    "    print(\"WARNING: Using CPU will cause slower train times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946a977fabb03bfd",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e44458ac84594557",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T01:59:53.884712Z",
     "start_time": "2024-07-24T01:59:53.882438Z"
    }
   },
   "outputs": [],
   "source": [
    "image_size = 256\n",
    "content_filename = 'cityscape.png'\n",
    "style_filename = 'cathedral.png'\n",
    "\n",
    "Neural_Style_Layer_List = ['0', '5', '10', '19', '28']\n",
    "\n",
    "Normalization_Method = 'None' # 'None' or 'imagenet'\n",
    "loss_method = 'Base' # 'Base', 'Perceptual', 'Wasserstein'\n",
    "\n",
    "image_save_folder = 'Base_cityscape_cathedral_2'\n",
    "\n",
    "total_steps = 3100\n",
    "save_steps = 100\n",
    "learning_rate = 0.001 # loss for base 0.01\n",
    "alpha = 1 \n",
    "beta = 0.01 # beta for base 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "788b5e3530a2f19",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T01:59:58.135666Z",
     "start_time": "2024-07-24T01:59:58.127974Z"
    }
   },
   "outputs": [],
   "source": [
    "# if image_save_folder does not exist, create it\n",
    "import os\n",
    "if not os.path.exists('final_project_gen_images/' + image_save_folder):\n",
    "    os.makedirs('final_project_gen_images/' + image_save_folder)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e811622b30dbd33c",
   "metadata": {},
   "source": [
    "# Neural Style Transfer Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e84bd582d033b75",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T01:59:59.285692Z",
     "start_time": "2024-07-24T01:59:59.281178Z"
    }
   },
   "outputs": [],
   "source": [
    "class VGG19(nn.Module):\n",
    "    def __init__(self, layer_list):\n",
    "        super(VGG19, self).__init__()\n",
    "        self.chosen_features = layer_list\n",
    "        self.model = models.vgg19(pretrained=True).features[:29]\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        for layer_num, layer in enumerate(self.model):\n",
    "            x = layer(x)\n",
    "            if str(layer_num) in self.chosen_features:\n",
    "                features.append(x)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd6f6578361c296f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T02:00:00.440473Z",
     "start_time": "2024-07-24T01:59:59.856361Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rlepko/PycharmProjects/GT_DeepLearning/Final_Project/venv/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/rlepko/PycharmProjects/GT_DeepLearning/Final_Project/venv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "ns_model = VGG19(Neural_Style_Layer_List).to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd10f61d4193c764",
   "metadata": {},
   "source": [
    "# perceptual Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4558dfb246fd29",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T02:00:01.106530Z",
     "start_time": "2024-07-24T02:00:00.569103Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rlepko/PycharmProjects/GT_DeepLearning/Final_Project/venv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "per_model = models.vgg16(pretrained=True).features.to(device).eval()\n",
    "# per_model = nn.Sequential(*list(per_model.children())[:-1])  # Remove the classification layers\n",
    "# per_model.eval()  # Set to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee42394425e41832",
   "metadata": {},
   "source": [
    "# Load Image Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8073ad1fbad94ee",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T02:00:01.890105Z",
     "start_time": "2024-07-24T02:00:01.883972Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_image(image_name, image_size=256):\n",
    "    if Normalization_Method == 'None':\n",
    "        loader = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0, 0, 0], std=[1, 1, 1])\n",
    "    ])\n",
    "        \n",
    "    elif Normalization_Method == 'imagenet':\n",
    "        loader = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    image = Image.open(image_name).convert('RGB')\n",
    "    image = loader(image).unsqueeze(0)\n",
    "    return image.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58656eab595e85ff",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T02:00:05.417602Z",
     "start_time": "2024-07-24T02:00:05.415369Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b3bfb24d8094999",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T02:00:06.070817Z",
     "start_time": "2024-07-24T02:00:06.066606Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load pre-trained VGG16 model for perceptual loss\n",
    "class VGG16Features(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16Features, self).__init__()\n",
    "        vgg16_model = models.vgg16(pretrained=True).features\n",
    "        self.layers = nn.Sequential(\n",
    "            vgg16_model[0], vgg16_model[1], vgg16_model[2], vgg16_model[3],\n",
    "            vgg16_model[4], vgg16_model[5], vgg16_model[6], vgg16_model[7]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Function to extract features using VGG16\n",
    "def extract_features(img, model, device):\n",
    "    model = model.to(device)\n",
    "    img = img.to(device)\n",
    "    with torch.no_grad():\n",
    "        features = model(img)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73948fff0c65df2",
   "metadata": {},
   "source": [
    "# Neural Style Transfer Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48be3497bacec44",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T02:00:07.073594Z",
     "start_time": "2024-07-24T02:00:07.067759Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93a75cccb160d4d8",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T02:00:07.908685Z",
     "start_time": "2024-07-24T02:00:07.427291Z"
    }
   },
   "outputs": [],
   "source": [
    "content = load_image(\"final_project_images/content_images/\" + content_filename, image_size)\n",
    "style = load_image(\"final_project_images/style_images/\" + style_filename, image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3aaf332a93b078f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T02:00:12.069286Z",
     "start_time": "2024-07-24T02:00:11.245447Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract features for perceptual loss\n",
    "per_model = VGG16Features().to(device).eval()\n",
    "content_features_per = extract_features(content, per_model, device)\n",
    "style_features_per = extract_features(style, per_model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99ce056c6bbc3730",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T02:00:12.074733Z",
     "start_time": "2024-07-24T02:00:12.069675Z"
    }
   },
   "outputs": [],
   "source": [
    "generated_image = content.clone().requires_grad_(True).to(device)\n",
    "optimizer = optim.Adam([generated_image], lr=learning_rate, betas=[0.5, 0.999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a637cdff191bd8f7",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T02:13:20.992610Z",
     "start_time": "2024-07-24T02:00:14.562217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: 445073.0\n",
      "step 100: 34144.75390625\n",
      "step 200: 15803.7099609375\n",
      "step 300: 10187.419921875\n",
      "step 400: 7609.453125\n",
      "step 500: 6153.7275390625\n",
      "step 600: 5182.7578125\n",
      "step 700: 4483.5576171875\n",
      "step 800: 3956.544921875\n",
      "step 900: 3545.655029296875\n",
      "step 1000: 3211.584228515625\n",
      "step 1100: 2940.3681640625\n",
      "step 1200: 2721.10693359375\n",
      "step 1300: 2539.415771484375\n",
      "step 1400: 2384.802490234375\n",
      "step 1500: 2252.59423828125\n",
      "step 1600: 2138.006103515625\n",
      "step 1700: 2038.57861328125\n",
      "step 1800: 1949.7105712890625\n",
      "step 1900: 1870.45166015625\n",
      "step 2000: 1798.5089111328125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 38\u001B[0m\n\u001B[1;32m     36\u001B[0m total_loss \u001B[38;5;241m=\u001B[39m alpha\u001B[38;5;241m*\u001B[39mcontent_loss \u001B[38;5;241m+\u001B[39m beta\u001B[38;5;241m*\u001B[39mstyle_loss\n\u001B[1;32m     37\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 38\u001B[0m \u001B[43mtotal_loss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     39\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m step \u001B[38;5;241m%\u001B[39m  save_steps \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/PycharmProjects/GT_DeepLearning/Final_Project/venv/lib/python3.9/site-packages/torch/_tensor.py:525\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    515\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    517\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    518\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    523\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    524\u001B[0m     )\n\u001B[0;32m--> 525\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    526\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    527\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/GT_DeepLearning/Final_Project/venv/lib/python3.9/site-packages/torch/autograd/__init__.py:267\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    262\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    264\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    266\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 267\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    275\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/GT_DeepLearning/Final_Project/venv/lib/python3.9/site-packages/torch/autograd/graph.py:744\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    742\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    743\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 744\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    745\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    746\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    747\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    748\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "loss_values = []\n",
    "step_list = []\n",
    "for step in range(total_steps):\n",
    "    generated_features = ns_model(generated_image)\n",
    "    content_features = ns_model(content)\n",
    "    style_features = ns_model(style)\n",
    "    \n",
    "    style_loss = 0\n",
    "    content_loss = 0\n",
    "    \n",
    "    for gen_feature, content_feature, style_feature in zip(generated_features,content_features, style_features):\n",
    "        batch_size, channel, height, width = gen_feature.shape\n",
    "        \n",
    "    \n",
    "        # Compute Gram Matrix\n",
    "        G_gen = gen_feature.view(channel, height*width).mm(gen_feature.view(channel, height*width).t())\n",
    "        G_style = style_feature.view(channel, height*width).mm(style_feature.view(channel, height*width).t())\n",
    "        \n",
    "        # Compute Loss\n",
    "        if loss_method == 'Base':\n",
    "            content_loss += torch.mean((gen_feature - content_feature)**2)\n",
    "            style_loss += torch.mean((G_gen - G_style)**2)\n",
    "        elif loss_method == 'Perceptual':\n",
    "            gen_features_per = per_model(generated_image)\n",
    "            content_loss += 100 * torch.mean((gen_features_per - content_features_per) ** 2)\n",
    "            style_loss += 100 * torch.mean((gen_features_per - style_features_per) ** 2)\n",
    "        elif loss_method == 'Wasserstein':\n",
    "            content_loss += torch.mean(generated_image) - torch.mean(content)\n",
    "            style_loss += torch.mean(generated_image) - torch.mean(style)\n",
    "        elif loss_method == 'total_variation':\n",
    "            content_loss += torch.mean(torch.abs(generated_image[:, :, :, 1:] - generated_image[:, :, :, :-1])) + torch.mean(torch.abs(generated_image[:, :, 1:, :] - generated_image[:, :, :-1, :]))\n",
    "            style_loss += torch.mean(torch.abs(generated_image[:, :, :, 1:] - generated_image[:, :, :, :-1])) + torch.mean(torch.abs(generated_image[:, :, 1:, :] - generated_image[:, :, :-1, :]))\n",
    "            \n",
    "       \n",
    "        \n",
    "    total_loss = alpha*content_loss + beta*style_loss\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step %  save_steps == 0:\n",
    "        loss_value = total_loss.item()\n",
    "        loss_values.append(loss_value)\n",
    "        step_list.append(step)\n",
    "        print(f'step {step}: {loss_value}')\n",
    "        image_name = \"final_project_gen_images/\" + image_save_folder + \"/\" + str(step) + \".png\"\n",
    "        save_image(generated_image, image_name)\n",
    "        \n",
    "# Save loss values\n",
    "print('saving loss values')\n",
    "df = pd.DataFrame(list(zip(step_list, loss_values)), columns =['Step', 'Loss'])\n",
    "df.to_csv(\"final_project_gen_images/\" + image_save_folder + \"/loss_values.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c11a0691c47c0aa",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
