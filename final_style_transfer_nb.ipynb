{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-23T19:20:05.291893Z",
     "start_time": "2024-07-23T19:20:05.288100Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.utils import save_image\n",
    "import pandas as pd\n",
    "\n",
    "import ssl\n",
    "import certifi\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device = mps\n"
     ]
    }
   ],
   "source": [
    "device = 'mps' if torch.backends.mps.is_available() else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device = \" + device)\n",
    "if device == 'cpu':\n",
    "    print(\"WARNING: Using CPU will cause slower train times\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T19:20:05.972626Z",
     "start_time": "2024-07-23T19:20:05.960362Z"
    }
   },
   "id": "306617d55ec8f23c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Variables"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "946a977fabb03bfd"
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "outputs": [],
   "source": [
    "image_size = 256\n",
    "content_filename = 'cityscape.png'\n",
    "style_filename = 'cathedral.png'\n",
    "\n",
    "# image_save_folder = 'perceptual_cityscape_cathedral'\n",
    "\n",
    "Neural_Style_Layer_List = ['0', '5', '10', '19', '28']\n",
    "\n",
    "Normalization_Method = 'None' # 'None' or 'imagenet'\n",
    "loss_method = 'Base' # 'Base', 'Perceptual', 'Wasserstein'\n",
    "\n",
    "image_save_folder = 'Base_cityscape_cathedral_1'\n",
    "\n",
    "total_steps = 3100\n",
    "save_steps = 100\n",
    "learning_rate = 0.001 # loss for base 0.01\n",
    "alpha = 1 \n",
    "beta = 0.01 # beta for base 0.01"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T01:41:09.987241Z",
     "start_time": "2024-07-24T01:41:09.977267Z"
    }
   },
   "id": "e44458ac84594557"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Base model\n",
    "miley-cathedral: learning rate 0.01, alpha 1, beta 0.01\n",
    "\n",
    "perception model\n",
    "gatys images: learning rate 0.0001, alpha 1, beta 1\n",
    "miley_zaha_1: learning rate 0.0001, alpha 1, beta 1\n",
    "miley_zaha_2: learning rate 0.0005, alpha 1, beta 1\n",
    "miley_zaha_3: learning rate 0.001, alpha 1, beta 1\n",
    "miley_zaha_4: learning rate 0.001, alpha 1, beta 1 steps: 6100\n",
    "miley_xoco: learning rate 0.001, alpha 1, beta 1 steps: 3100\n",
    "miley_cathedral: learning rate 0.001, alpha 1, beta 1 steps: 3100\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52779697679500fc"
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "outputs": [],
   "source": [
    "# if image_save_folder does not exist, create it\n",
    "import os\n",
    "if not os.path.exists('final_project_gen_images/' + image_save_folder):\n",
    "    os.makedirs('final_project_gen_images/' + image_save_folder)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T01:41:11.498275Z",
     "start_time": "2024-07-24T01:41:11.494506Z"
    }
   },
   "id": "788b5e3530a2f19"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Style Transfer Network"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e811622b30dbd33c"
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "outputs": [],
   "source": [
    "class VGG19(nn.Module):\n",
    "    def __init__(self, layer_list):\n",
    "        super(VGG19, self).__init__()\n",
    "        self.chosen_features = layer_list\n",
    "        self.model = models.vgg19(pretrained=True).features[:29]\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        for layer_num, layer in enumerate(self.model):\n",
    "            x = layer(x)\n",
    "            if str(layer_num) in self.chosen_features:\n",
    "                features.append(x)\n",
    "        return features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T01:41:12.253712Z",
     "start_time": "2024-07-24T01:41:12.249591Z"
    }
   },
   "id": "9e84bd582d033b75"
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rlepko/PycharmProjects/GT_DeepLearning/Final_Project/venv/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/rlepko/PycharmProjects/GT_DeepLearning/Final_Project/venv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "ns_model = VGG19(Neural_Style_Layer_List).to(device).eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T01:41:13.367503Z",
     "start_time": "2024-07-24T01:41:12.841274Z"
    }
   },
   "id": "dd6f6578361c296f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# perceptual Neural Network"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd10f61d4193c764"
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rlepko/PycharmProjects/GT_DeepLearning/Final_Project/venv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "per_model = models.vgg16(pretrained=True).features.to(device).eval()\n",
    "# per_model = nn.Sequential(*list(per_model.children())[:-1])  # Remove the classification layers\n",
    "# per_model.eval()  # Set to evaluation mode"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T01:41:13.883364Z",
     "start_time": "2024-07-24T01:41:13.368639Z"
    }
   },
   "id": "b4558dfb246fd29"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Image Function"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee42394425e41832"
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "outputs": [],
   "source": [
    "def load_image(image_name, image_size=256):\n",
    "    if Normalization_Method == 'None':\n",
    "        loader = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0, 0, 0], std=[1, 1, 1])\n",
    "    ])\n",
    "        \n",
    "    elif Normalization_Method == 'imagenet':\n",
    "        loader = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    image = Image.open(image_name).convert('RGB')\n",
    "    image = loader(image).unsqueeze(0)\n",
    "    return image.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T01:41:13.888646Z",
     "start_time": "2024-07-24T01:41:13.885849Z"
    }
   },
   "id": "8073ad1fbad94ee"
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T01:41:14.469023Z",
     "start_time": "2024-07-24T01:41:14.467806Z"
    }
   },
   "id": "58656eab595e85ff"
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "outputs": [],
   "source": [
    "# Load pre-trained VGG16 model for perceptual loss\n",
    "class VGG16Features(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16Features, self).__init__()\n",
    "        vgg16_model = models.vgg16(pretrained=True).features\n",
    "        self.layers = nn.Sequential(\n",
    "            vgg16_model[0], vgg16_model[1], vgg16_model[2], vgg16_model[3],\n",
    "            vgg16_model[4], vgg16_model[5], vgg16_model[6], vgg16_model[7]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Function to extract features using VGG16\n",
    "def extract_features(img, model, device):\n",
    "    model = model.to(device)\n",
    "    img = img.to(device)\n",
    "    with torch.no_grad():\n",
    "        features = model(img)\n",
    "    return features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T01:41:14.473886Z",
     "start_time": "2024-07-24T01:41:14.471754Z"
    }
   },
   "id": "2b3bfb24d8094999"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Style Transfer Code"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a73948fff0c65df2"
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "outputs": [],
   "source": [
    "content = load_image(\"final_project_images/content_images/\" + content_filename, image_size)\n",
    "style = load_image(\"final_project_images/style_images/\" + style_filename, image_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T01:41:15.419156Z",
     "start_time": "2024-07-24T01:41:15.053694Z"
    }
   },
   "id": "93a75cccb160d4d8"
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rlepko/PycharmProjects/GT_DeepLearning/Final_Project/venv/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/rlepko/PycharmProjects/GT_DeepLearning/Final_Project/venv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Extract features for perceptual loss\n",
    "per_model = VGG16Features().to(device).eval()\n",
    "content_features_per = extract_features(content, per_model, device)\n",
    "style_features_per = extract_features(style, per_model, device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T01:41:15.934824Z",
     "start_time": "2024-07-24T01:41:15.420690Z"
    }
   },
   "id": "b3aaf332a93b078f"
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "outputs": [],
   "source": [
    "generated_image = content.clone().requires_grad_(True).to(device)\n",
    "optimizer = optim.Adam([generated_image], lr=learning_rate, betas=[0.5, 0.999])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T01:41:15.939913Z",
     "start_time": "2024-07-24T01:41:15.936447Z"
    }
   },
   "id": "99ce056c6bbc3730"
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: 445073.0\n",
      "step 100: 34144.75390625\n",
      "step 200: 15803.7099609375\n",
      "step 300: 10187.419921875\n",
      "step 400: 7609.453125\n",
      "step 500: 6153.7275390625\n",
      "step 600: 5182.7578125\n",
      "step 700: 4483.5576171875\n",
      "step 800: 3956.544921875\n",
      "step 900: 3545.655029296875\n",
      "step 1000: 3211.584228515625\n",
      "step 1100: 2940.3681640625\n",
      "step 1200: 2721.10693359375\n",
      "step 1300: 2539.415771484375\n",
      "step 1400: 2384.802490234375\n",
      "step 1500: 2252.59423828125\n",
      "step 1600: 2138.006103515625\n",
      "step 1700: 2038.57861328125\n",
      "step 1800: 1949.7105712890625\n",
      "step 1900: 1870.45166015625\n",
      "step 2000: 1798.5089111328125\n",
      "step 2100: 1732.3524169921875\n",
      "step 2200: 1670.1007080078125\n",
      "step 2300: 1619.201171875\n",
      "step 2400: 1557.1453857421875\n",
      "step 2500: 1507.616455078125\n",
      "step 2600: 1462.5784912109375\n",
      "step 2700: 1415.2537841796875\n",
      "step 2800: 1380.593017578125\n",
      "step 2900: 1330.768310546875\n",
      "step 3000: 1291.947265625\n",
      "saving loss values\n"
     ]
    }
   ],
   "source": [
    "loss_values = []\n",
    "step_list = []\n",
    "for step in range(total_steps):\n",
    "    generated_features = ns_model(generated_image)\n",
    "    content_features = ns_model(content)\n",
    "    style_features = ns_model(style)\n",
    "    \n",
    "    style_loss = 0\n",
    "    content_loss = 0\n",
    "    \n",
    "    for gen_feature, content_feature, style_feature in zip(generated_features,content_features, style_features):\n",
    "        batch_size, channel, height, width = gen_feature.shape\n",
    "        \n",
    "    \n",
    "        # Compute Gram Matrix\n",
    "        G_gen = gen_feature.view(channel, height*width).mm(gen_feature.view(channel, height*width).t())\n",
    "        G_style = style_feature.view(channel, height*width).mm(style_feature.view(channel, height*width).t())\n",
    "        \n",
    "        # Compute Loss\n",
    "        if loss_method == 'Base':\n",
    "            content_loss += torch.mean((gen_feature - content_feature)**2)\n",
    "            style_loss += torch.mean((G_gen - G_style)**2)\n",
    "        elif loss_method == 'Perceptual':\n",
    "            gen_features_per = per_model(generated_image)\n",
    "            content_loss += 100 * torch.mean((gen_features_per - content_features_per) ** 2)\n",
    "            style_loss += 100 * torch.mean((gen_features_per - style_features_per) ** 2)\n",
    "        elif loss_method == 'Wasserstein':\n",
    "            content_loss += torch.mean(generated_image) - torch.mean(content)\n",
    "            style_loss += torch.mean(generated_image) - torch.mean(style)\n",
    "        elif loss_method == 'total_variation':\n",
    "            content_loss += torch.mean(torch.abs(generated_image[:, :, :, 1:] - generated_image[:, :, :, :-1])) + torch.mean(torch.abs(generated_image[:, :, 1:, :] - generated_image[:, :, :-1, :]))\n",
    "            style_loss += torch.mean(torch.abs(generated_image[:, :, :, 1:] - generated_image[:, :, :, :-1])) + torch.mean(torch.abs(generated_image[:, :, 1:, :] - generated_image[:, :, :-1, :]))\n",
    "            \n",
    "       \n",
    "        \n",
    "    total_loss = alpha*content_loss + beta*style_loss\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step %  save_steps == 0:\n",
    "        loss_value = total_loss.item()\n",
    "        loss_values.append(loss_value)\n",
    "        step_list.append(step)\n",
    "        print(f'step {step}: {loss_value}')\n",
    "        image_name = \"final_project_gen_images/\" + image_save_folder + \"/\" + str(step) + \".png\"\n",
    "        save_image(generated_image, image_name)\n",
    "        \n",
    "# Save loss values\n",
    "print('saving loss values')\n",
    "df = pd.DataFrame(list(zip(step_list, loss_values)), columns =['Step', 'Loss'])\n",
    "df.to_csv(\"final_project_gen_images/\" + image_save_folder + \"/loss_values.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T01:51:01.497689Z",
     "start_time": "2024-07-24T01:41:16.679546Z"
    }
   },
   "id": "a637cdff191bd8f7"
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T00:30:52.125431Z",
     "start_time": "2024-07-24T00:30:52.123342Z"
    }
   },
   "id": "1c11a0691c47c0aa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9954f3d680c01754"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
