{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-23T19:20:05.291893Z",
     "start_time": "2024-07-23T19:20:05.288100Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.utils import save_image\n",
    "import pandas as pd\n",
    "\n",
    "import ssl\n",
    "import certifi\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device = mps\n"
     ]
    }
   ],
   "source": [
    "device = 'mps' if torch.backends.mps.is_available() else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device = \" + device)\n",
    "if device == 'cpu':\n",
    "    print(\"WARNING: Using CPU will cause slower train times\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T19:20:05.972626Z",
     "start_time": "2024-07-23T19:20:05.960362Z"
    }
   },
   "id": "306617d55ec8f23c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Variables"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "946a977fabb03bfd"
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "image_size = 256\n",
    "content_filename = 'cityscape.png'\n",
    "style_filename = 'cathedral.png'\n",
    "\n",
    "# image_save_folder = 'perceptual_cityscape_cathedral'\n",
    "\n",
    "Neural_Style_Layer_List = ['0', '5', '10', '19', '28']\n",
    "\n",
    "Normalization_Method = 'None' # 'None' or 'imagenet'\n",
    "loss_method = 'Perceptual' # 'Base', 'Perceptual', 'Wasserstein'\n",
    "\n",
    "image_save_folder = 'perceptual_cityscape_cathedral'\n",
    "\n",
    "total_steps = 3100\n",
    "save_steps = 100\n",
    "learning_rate = 0.0001 # loss for base 0.01\n",
    "alpha = 1 \n",
    "beta = 1 # beta for base 0.01"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T19:27:26.568476Z",
     "start_time": "2024-07-23T19:27:26.566785Z"
    }
   },
   "id": "e44458ac84594557"
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "# if image_save_folder does not exist, create it\n",
    "import os\n",
    "if not os.path.exists('final_project_gen_images/' + image_save_folder):\n",
    "    os.makedirs('final_project_gen_images/' + image_save_folder)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T19:27:27.532281Z",
     "start_time": "2024-07-23T19:27:27.522681Z"
    }
   },
   "id": "788b5e3530a2f19"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Style Transfer Network"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e811622b30dbd33c"
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "class VGG19(nn.Module):\n",
    "    def __init__(self, layer_list):\n",
    "        super(VGG19, self).__init__()\n",
    "        self.chosen_features = layer_list\n",
    "        self.model = models.vgg19(pretrained=True).features[:29]\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        for layer_num, layer in enumerate(self.model):\n",
    "            x = layer(x)\n",
    "            if str(layer_num) in self.chosen_features:\n",
    "                features.append(x)\n",
    "        return features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T19:27:40.299484Z",
     "start_time": "2024-07-23T19:27:40.293431Z"
    }
   },
   "id": "9e84bd582d033b75"
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rlepko/PycharmProjects/GT_DeepLearning/Final_Project/venv/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/rlepko/PycharmProjects/GT_DeepLearning/Final_Project/venv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "ns_model = VGG19(Neural_Style_Layer_List).to(device).eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T19:27:41.500619Z",
     "start_time": "2024-07-23T19:27:40.968548Z"
    }
   },
   "id": "dd6f6578361c296f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# perceptual Neural Network"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd10f61d4193c764"
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rlepko/PycharmProjects/GT_DeepLearning/Final_Project/venv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "per_model = models.vgg16(pretrained=True).features.to(device).eval()\n",
    "# per_model = nn.Sequential(*list(per_model.children())[:-1])  # Remove the classification layers\n",
    "# per_model.eval()  # Set to evaluation mode"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T19:27:42.078636Z",
     "start_time": "2024-07-23T19:27:41.495225Z"
    }
   },
   "id": "b4558dfb246fd29"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Image Function"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee42394425e41832"
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "def load_image(image_name, image_size=256):\n",
    "    if Normalization_Method == 'None':\n",
    "        loader = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0, 0, 0], std=[1, 1, 1])\n",
    "    ])\n",
    "        \n",
    "    elif Normalization_Method == 'imagenet':\n",
    "        loader = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    image = Image.open(image_name).convert('RGB')\n",
    "    image = loader(image).unsqueeze(0)\n",
    "    return image.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T19:30:08.978471Z",
     "start_time": "2024-07-23T19:30:08.970847Z"
    }
   },
   "id": "8073ad1fbad94ee"
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T19:30:09.887879Z",
     "start_time": "2024-07-23T19:30:09.885894Z"
    }
   },
   "id": "58656eab595e85ff"
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "# Load pre-trained VGG16 model for perceptual loss\n",
    "class VGG16Features(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16Features, self).__init__()\n",
    "        vgg16_model = models.vgg16(pretrained=True).features\n",
    "        self.layers = nn.Sequential(\n",
    "            vgg16_model[0], vgg16_model[1], vgg16_model[2], vgg16_model[3],\n",
    "            vgg16_model[4], vgg16_model[5], vgg16_model[6], vgg16_model[7]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Function to extract features using VGG16\n",
    "def extract_features(img, model, device):\n",
    "    model = model.to(device)\n",
    "    img = img.to(device)\n",
    "    with torch.no_grad():\n",
    "        features = model(img)\n",
    "    return features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T19:30:10.574342Z",
     "start_time": "2024-07-23T19:30:10.572245Z"
    }
   },
   "id": "2b3bfb24d8094999"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Style Transfer Code"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a73948fff0c65df2"
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T19:30:11.301293Z",
     "start_time": "2024-07-23T19:30:11.296308Z"
    }
   },
   "id": "48be3497bacec44"
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "content = load_image(\"final_project_images/content_images/\" + content_filename, image_size)\n",
    "style = load_image(\"final_project_images/style_images/\" + style_filename, image_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T19:30:12.242261Z",
     "start_time": "2024-07-23T19:30:11.863874Z"
    }
   },
   "id": "93a75cccb160d4d8"
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rlepko/PycharmProjects/GT_DeepLearning/Final_Project/venv/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/rlepko/PycharmProjects/GT_DeepLearning/Final_Project/venv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Extract features for perceptual loss\n",
    "per_model = VGG16Features().to(device).eval()\n",
    "content_features_per = extract_features(content, per_model, device)\n",
    "style_features_per = extract_features(style, per_model, device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T19:30:13.497707Z",
     "start_time": "2024-07-23T19:30:12.952534Z"
    }
   },
   "id": "b3aaf332a93b078f"
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "generated_image = content.clone().requires_grad_(True).to(device)\n",
    "optimizer = optim.Adam([generated_image], lr=learning_rate, betas=[0.5, 0.999])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T19:30:14.126149Z",
     "start_time": "2024-07-23T19:30:14.123497Z"
    }
   },
   "id": "99ce056c6bbc3730"
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: 5644.9384765625\n",
      "step 100: 4887.3173828125\n",
      "step 200: 4427.0869140625\n",
      "step 300: 4147.41064453125\n",
      "step 400: 3956.109375\n",
      "step 500: 3816.333984375\n",
      "step 600: 3709.689453125\n",
      "step 700: 3625.794921875\n",
      "step 800: 3558.0947265625\n",
      "step 900: 3502.44775390625\n",
      "step 1000: 3455.856689453125\n",
      "step 1100: 3416.4052734375\n",
      "step 1200: 3382.4970703125\n",
      "step 1300: 3352.92041015625\n",
      "step 1400: 3327.178955078125\n",
      "step 1500: 3304.451171875\n",
      "step 1600: 3284.17578125\n",
      "step 1700: 3266.0703125\n",
      "step 1800: 3249.911376953125\n",
      "step 1900: 3235.38623046875\n",
      "step 2000: 3222.265625\n",
      "step 2100: 3210.44189453125\n",
      "step 2200: 3199.79345703125\n",
      "step 2300: 3190.123046875\n",
      "step 2400: 3181.20947265625\n",
      "step 2500: 3173.14892578125\n",
      "step 2600: 3165.79296875\n",
      "step 2700: 3159.01953125\n",
      "step 2800: 3152.7568359375\n",
      "step 2900: 3146.99267578125\n",
      "step 3000: 3141.72607421875\n",
      "saving loss values\n"
     ]
    }
   ],
   "source": [
    "loss_values = []\n",
    "step_list = []\n",
    "for step in range(total_steps):\n",
    "    generated_features = ns_model(generated_image)\n",
    "    content_features = ns_model(content)\n",
    "    style_features = ns_model(style)\n",
    "    \n",
    "    style_loss = 0\n",
    "    content_loss = 0\n",
    "    \n",
    "    for gen_feature, content_feature, style_feature in zip(generated_features,content_features, style_features):\n",
    "        batch_size, channel, height, width = gen_feature.shape\n",
    "        \n",
    "    \n",
    "        # Compute Gram Matrix\n",
    "        G_gen = gen_feature.view(channel, height*width).mm(gen_feature.view(channel, height*width).t())\n",
    "        G_style = style_feature.view(channel, height*width).mm(style_feature.view(channel, height*width).t())\n",
    "        \n",
    "        # Compute Loss\n",
    "        if loss_method == 'Base':\n",
    "            content_loss += torch.mean((gen_feature - content_feature)**2)\n",
    "            style_loss += torch.mean((G_gen - G_style)**2)\n",
    "        elif loss_method == 'Perceptual':\n",
    "            gen_features_per = per_model(generated_image)\n",
    "            content_loss += 100 * torch.mean((gen_features_per - content_features_per) ** 2)\n",
    "            style_loss += 100 * torch.mean((gen_features_per - style_features_per) ** 2)\n",
    "        elif loss_method == 'Wasserstein':\n",
    "            content_loss += torch.mean(generated_image) - torch.mean(content)\n",
    "            style_loss += torch.mean(generated_image) - torch.mean(style)\n",
    "        elif loss_method == 'total_variation':\n",
    "            content_loss += torch.mean(torch.abs(generated_image[:, :, :, 1:] - generated_image[:, :, :, :-1])) + torch.mean(torch.abs(generated_image[:, :, 1:, :] - generated_image[:, :, :-1, :]))\n",
    "            style_loss += torch.mean(torch.abs(generated_image[:, :, :, 1:] - generated_image[:, :, :, :-1])) + torch.mean(torch.abs(generated_image[:, :, 1:, :] - generated_image[:, :, :-1, :]))\n",
    "            \n",
    "       \n",
    "        \n",
    "    total_loss = alpha*content_loss + beta*style_loss\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step %  save_steps == 0:\n",
    "        loss_value = total_loss.item()\n",
    "        loss_values.append(loss_value)\n",
    "        step_list.append(step)\n",
    "        print(f'step {step}: {loss_value}')\n",
    "        image_name = \"final_project_gen_images/\" + image_save_folder + \"/\" + str(step) + \".png\"\n",
    "        save_image(generated_image, image_name)\n",
    "        \n",
    "# Save loss values\n",
    "print('saving loss values')\n",
    "df = pd.DataFrame(list(zip(step_list, loss_values)), columns =['Step', 'Loss'])\n",
    "df.to_csv(\"final_project_gen_images/\" + image_save_folder + \"/loss_values.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T19:38:12.338268Z",
     "start_time": "2024-07-23T19:30:14.775311Z"
    }
   },
   "id": "a637cdff191bd8f7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1c11a0691c47c0aa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
