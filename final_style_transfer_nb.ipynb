{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-21T23:23:10.626014Z",
     "start_time": "2024-07-21T23:23:09.444737Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.utils import save_image\n",
    "import pandas as pd\n",
    "\n",
    "import ssl\n",
    "import certifi\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device = mps\n"
     ]
    }
   ],
   "source": [
    "device = 'mps' if torch.backends.mps.is_available() else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device = \" + device)\n",
    "if device == 'cpu':\n",
    "    print(\"WARNING: Using CPU will cause slower train times\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-21T23:23:12.271410Z",
     "start_time": "2024-07-21T23:23:12.268074Z"
    }
   },
   "id": "306617d55ec8f23c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Variables"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "946a977fabb03bfd"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "image_size = 256\n",
    "content_filename = 'Tuebingen_Neckarfront.jpg'\n",
    "style_filename = 'Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg'\n",
    "\n",
    "image_save_folder = 'gatys_original_images_2'\n",
    "\n",
    "Neural_Style_Layer_List = ['0', '5', '10', '19', '28']\n",
    "\n",
    "Normalization_Method = 'None' # 'None' or 'imagenet'\n",
    "loss_method = 'Perceptual' # 'Base', 'Perceptual', 'Wasserstein'\n",
    "\n",
    "image_save_folder = 'gatys_original_images_2' + '_' + loss_method\n",
    "\n",
    "total_steps = 3100\n",
    "save_steps = 100\n",
    "learning_rate = 0.0001 # loss for base 0.01\n",
    "alpha = 1 \n",
    "beta = 1 # beta for base 0.01"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T13:17:48.984395Z",
     "start_time": "2024-07-23T13:17:48.978534Z"
    }
   },
   "id": "e44458ac84594557"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "# if image_save_folder does not exist, create it\n",
    "import os\n",
    "if not os.path.exists('final_project_gen_images/' + image_save_folder):\n",
    "    os.makedirs('final_project_gen_images/' + image_save_folder)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T13:17:49.746064Z",
     "start_time": "2024-07-23T13:17:49.742830Z"
    }
   },
   "id": "788b5e3530a2f19"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Style Transfer Network"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e811622b30dbd33c"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "class VGG19(nn.Module):\n",
    "    def __init__(self, layer_list):\n",
    "        super(VGG19, self).__init__()\n",
    "        self.chosen_features = layer_list\n",
    "        self.model = models.vgg19(pretrained=True).features[:29]\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        for layer_num, layer in enumerate(self.model):\n",
    "            x = layer(x)\n",
    "            if str(layer_num) in self.chosen_features:\n",
    "                features.append(x)\n",
    "        return features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T13:17:50.654861Z",
     "start_time": "2024-07-23T13:17:50.650493Z"
    }
   },
   "id": "9e84bd582d033b75"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rlepko/PycharmProjects/GT_DeepLearning/Final_Project/venv/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/rlepko/PycharmProjects/GT_DeepLearning/Final_Project/venv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "ns_model = VGG19(Neural_Style_Layer_List).to(device).eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T13:17:51.701429Z",
     "start_time": "2024-07-23T13:17:51.172081Z"
    }
   },
   "id": "dd6f6578361c296f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# perceptual Neural Network"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd10f61d4193c764"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rlepko/PycharmProjects/GT_DeepLearning/Final_Project/venv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "per_model = models.vgg16(pretrained=True).features.to(device).eval()\n",
    "# per_model = nn.Sequential(*list(per_model.children())[:-1])  # Remove the classification layers\n",
    "# per_model.eval()  # Set to evaluation mode"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T13:17:52.281552Z",
     "start_time": "2024-07-23T13:17:51.703972Z"
    }
   },
   "id": "b4558dfb246fd29"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Image Function"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee42394425e41832"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "def load_image(image_name, image_size=256):\n",
    "    if Normalization_Method == 'None':\n",
    "        loader = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0, 0, 0], std=[1, 1, 1])\n",
    "    ])\n",
    "        \n",
    "    elif Normalization_Method == 'imagenet':\n",
    "        loader = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    image = Image.open(image_name)\n",
    "    image = loader(image).unsqueeze(0)\n",
    "    return image.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T13:17:52.289221Z",
     "start_time": "2024-07-23T13:17:52.284128Z"
    }
   },
   "id": "8073ad1fbad94ee"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T13:17:52.832268Z",
     "start_time": "2024-07-23T13:17:52.830941Z"
    }
   },
   "id": "58656eab595e85ff"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "# Load pre-trained VGG16 model for perceptual loss\n",
    "class VGG16Features(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16Features, self).__init__()\n",
    "        vgg16_model = models.vgg16(pretrained=True).features\n",
    "        self.layers = nn.Sequential(\n",
    "            vgg16_model[0], vgg16_model[1], vgg16_model[2], vgg16_model[3],\n",
    "            vgg16_model[4], vgg16_model[5], vgg16_model[6], vgg16_model[7]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Function to extract features using VGG16\n",
    "def extract_features(img, model, device):\n",
    "    model = model.to(device)\n",
    "    img = img.to(device)\n",
    "    with torch.no_grad():\n",
    "        features = model(img)\n",
    "    return features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T13:17:53.342033Z",
     "start_time": "2024-07-23T13:17:53.340155Z"
    }
   },
   "id": "2b3bfb24d8094999"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Style Transfer Code"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a73948fff0c65df2"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T13:17:54.136999Z",
     "start_time": "2024-07-23T13:17:54.132553Z"
    }
   },
   "id": "48be3497bacec44"
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "content = load_image(\"gatys_original_images/\" + content_filename, image_size)\n",
    "style = load_image(\"gatys_original_images/\" + style_filename, image_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T13:17:54.662276Z",
     "start_time": "2024-07-23T13:17:54.637612Z"
    }
   },
   "id": "93a75cccb160d4d8"
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rlepko/PycharmProjects/GT_DeepLearning/Final_Project/venv/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/rlepko/PycharmProjects/GT_DeepLearning/Final_Project/venv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Extract features for perceptual loss\n",
    "per_model = VGG16Features().to(device).eval()\n",
    "content_features_per = extract_features(content, per_model, device)\n",
    "style_features_per = extract_features(style, per_model, device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T13:17:55.723024Z",
     "start_time": "2024-07-23T13:17:55.211898Z"
    }
   },
   "id": "b3aaf332a93b078f"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "generated_image = content.clone().requires_grad_(True).to(device)\n",
    "optimizer = optim.Adam([generated_image], lr=learning_rate, betas=[0.5, 0.999])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T13:17:55.727408Z",
     "start_time": "2024-07-23T13:17:55.724040Z"
    }
   },
   "id": "99ce056c6bbc3730"
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: 3726.54248046875\n",
      "step 100: 3110.81884765625\n",
      "step 200: 2788.247314453125\n",
      "step 300: 2602.488037109375\n",
      "step 400: 2479.765380859375\n",
      "step 500: 2393.5654296875\n",
      "step 600: 2329.72705078125\n",
      "step 700: 2280.6640625\n",
      "step 800: 2242.005126953125\n",
      "step 900: 2210.60302734375\n",
      "step 1000: 2184.590087890625\n",
      "step 1100: 2162.80322265625\n",
      "step 1200: 2144.36962890625\n",
      "step 1300: 2128.5390625\n",
      "step 1400: 2114.6953125\n",
      "step 1500: 2102.5888671875\n",
      "step 1600: 2091.94091796875\n",
      "step 1700: 2082.481689453125\n",
      "step 1800: 2073.969970703125\n",
      "step 1900: 2066.3115234375\n",
      "step 2000: 2059.37109375\n",
      "step 2100: 2053.06201171875\n",
      "step 2200: 2047.291259765625\n",
      "step 2300: 2042.024658203125\n",
      "step 2400: 2037.2115478515625\n",
      "step 2500: 2032.771484375\n",
      "step 2600: 2028.681396484375\n",
      "step 2700: 2024.907958984375\n",
      "step 2800: 2021.41748046875\n",
      "step 2900: 2018.19140625\n",
      "step 3000: 2015.191162109375\n",
      "saving loss values\n"
     ]
    }
   ],
   "source": [
    "loss_values = []\n",
    "step_list = []\n",
    "for step in range(total_steps):\n",
    "    generated_features = ns_model(generated_image)\n",
    "    content_features = ns_model(content)\n",
    "    style_features = ns_model(style)\n",
    "    \n",
    "    style_loss = 0\n",
    "    content_loss = 0\n",
    "    \n",
    "    for gen_feature, content_feature, style_feature in zip(generated_features,content_features, style_features):\n",
    "        batch_size, channel, height, width = gen_feature.shape\n",
    "        \n",
    "    \n",
    "        # Compute Gram Matrix\n",
    "        G_gen = gen_feature.view(channel, height*width).mm(gen_feature.view(channel, height*width).t())\n",
    "        G_style = style_feature.view(channel, height*width).mm(style_feature.view(channel, height*width).t())\n",
    "        \n",
    "        # Compute Loss\n",
    "        if loss_method == 'Base':\n",
    "            content_loss += torch.mean((gen_feature - content_feature)**2)\n",
    "            style_loss += torch.mean((G_gen - G_style)**2)\n",
    "        elif loss_method == 'Perceptual':\n",
    "            gen_features_per = per_model(generated_image)\n",
    "            content_loss += 100 * torch.mean((gen_features_per - content_features_per) ** 2)\n",
    "            style_loss += 100 * torch.mean((gen_features_per - style_features_per) ** 2)\n",
    "        elif loss_method == 'Wasserstein':\n",
    "            content_loss += torch.mean(generated_image) - torch.mean(content)\n",
    "            style_loss += torch.mean(generated_image) - torch.mean(style)\n",
    "        elif loss_method == 'total_variation':\n",
    "            content_loss += torch.mean(torch.abs(generated_image[:, :, :, 1:] - generated_image[:, :, :, :-1])) + torch.mean(torch.abs(generated_image[:, :, 1:, :] - generated_image[:, :, :-1, :]))\n",
    "            style_loss += torch.mean(torch.abs(generated_image[:, :, :, 1:] - generated_image[:, :, :, :-1])) + torch.mean(torch.abs(generated_image[:, :, 1:, :] - generated_image[:, :, :-1, :]))\n",
    "            \n",
    "       \n",
    "        \n",
    "    total_loss = alpha*content_loss + beta*style_loss\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step %  save_steps == 0:\n",
    "        loss_value = total_loss.item()\n",
    "        loss_values.append(loss_value)\n",
    "        step_list.append(step)\n",
    "        print(f'step {step}: {loss_value}')\n",
    "        image_name = \"final_project_gen_images/\" + image_save_folder + \"/\" + str(step) + \".png\"\n",
    "        save_image(generated_image, image_name)\n",
    "        \n",
    "# Save loss values\n",
    "print('saving loss values')\n",
    "df = pd.DataFrame(list(zip(step_list, loss_values)), columns =['Step', 'Loss'])\n",
    "df.to_csv(\"final_project_gen_images/\" + image_save_folder + \"/loss_values.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T13:25:48.525247Z",
     "start_time": "2024-07-23T13:17:55.732785Z"
    }
   },
   "id": "a637cdff191bd8f7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1c11a0691c47c0aa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
