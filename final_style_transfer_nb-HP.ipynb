{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T23:23:10.626014Z",
     "start_time": "2024-07-21T23:23:09.444737Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.utils import save_image\n",
    "import pandas as pd\n",
    "\n",
    "import ssl\n",
    "import certifi\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "306617d55ec8f23c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T23:23:12.271410Z",
     "start_time": "2024-07-21T23:23:12.268074Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device = cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'mps' if torch.backends.mps.is_available() else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device = \" + device)\n",
    "if device == 'cpu':\n",
    "    print(\"WARNING: Using CPU will cause slower train times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946a977fabb03bfd",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e44458ac84594557",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T13:17:48.984395Z",
     "start_time": "2024-07-23T13:17:48.978534Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "image_size = 256\n",
    "content_filename = 'miley_cyrus.jpg'\n",
    "style_filename = 'zaha-texture.png'\n",
    "\n",
    "image_save_folder = content_filename + '_' + style_filename\n",
    "\n",
    "Neural_Style_Layer_List = ['0', '5', '10', '19', '28']\n",
    "\n",
    "Normalization_Method = 'None' # 'None' or 'imagenet'\n",
    "CNN = 'Perceptual' # 'original', 'test1', 'test2', 'test3'\n",
    "loss_method = 'Base'\n",
    "\n",
    "image_save_folder = image_save_folder + '_' + CNN\n",
    "\n",
    "total_steps = 4100\n",
    "save_steps = 100\n",
    "learning_rate = 0.0001 # loss for base 0.01\n",
    "alpha = 1 \n",
    "beta = 1 # beta for base 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "788b5e3530a2f19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T13:17:49.746064Z",
     "start_time": "2024-07-23T13:17:49.742830Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# if image_save_folder does not exist, create it\n",
    "import os\n",
    "if not os.path.exists('final_project_gen_images/' + image_save_folder):\n",
    "    os.makedirs('final_project_gen_images/' + image_save_folder)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e811622b30dbd33c",
   "metadata": {},
   "source": [
    "# Neural Style Transfer Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9e84bd582d033b75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T13:17:50.654861Z",
     "start_time": "2024-07-23T13:17:50.650493Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class VGG19(nn.Module):\n",
    "    def __init__(self, layer_list):\n",
    "        super(VGG19, self).__init__()\n",
    "        self.chosen_features = layer_list\n",
    "        self.model = models.vgg19(pretrained=True).features[:29]\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        for layer_num, layer in enumerate(self.model):\n",
    "            x = layer(x)\n",
    "            if str(layer_num) in self.chosen_features:\n",
    "                features.append(x)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dd6f6578361c296f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T13:17:51.701429Z",
     "start_time": "2024-07-23T13:17:51.172081Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ns_model = VGG19(Neural_Style_Layer_List).to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd10f61d4193c764",
   "metadata": {},
   "source": [
    "# perceptual Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b4558dfb246fd29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T13:17:52.281552Z",
     "start_time": "2024-07-23T13:17:51.703972Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "per_model = models.vgg16(pretrained=True).features.to(device).eval()\n",
    "# per_model = nn.Sequential(*list(per_model.children())[:-1])  # Remove the classification layers\n",
    "# per_model.eval()  # Set to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee42394425e41832",
   "metadata": {},
   "source": [
    "# Load Image Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8073ad1fbad94ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T13:17:52.289221Z",
     "start_time": "2024-07-23T13:17:52.284128Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def load_image(image_name, image_size=256):\n",
    "    if Normalization_Method == 'None':\n",
    "        loader = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0, 0, 0], std=[1, 1, 1])\n",
    "    ])\n",
    "        \n",
    "    elif Normalization_Method == 'imagenet':\n",
    "        loader = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    image = Image.open(image_name)\n",
    "    image = image.convert('RGB')\n",
    "    image = loader(image).unsqueeze(0)\n",
    "    return image.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58656eab595e85ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T13:17:52.832268Z",
     "start_time": "2024-07-23T13:17:52.830941Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2b3bfb24d8094999",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T13:17:53.342033Z",
     "start_time": "2024-07-23T13:17:53.340155Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Load pre-trained VGG16 model for perceptual loss\n",
    "class VGG16Features(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16Features, self).__init__()\n",
    "        vgg16_model = models.vgg16(pretrained=True).features\n",
    "        self.layers = nn.Sequential(\n",
    "            vgg16_model[0], vgg16_model[1], vgg16_model[2], vgg16_model[3],\n",
    "            vgg16_model[4], vgg16_model[5], vgg16_model[6], vgg16_model[7]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Function to extract features using VGG16\n",
    "def extract_features(img, model, device):\n",
    "    model = model.to(device)\n",
    "    img = img.to(device)\n",
    "    with torch.no_grad():\n",
    "        features = model(img)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73948fff0c65df2",
   "metadata": {},
   "source": [
    "# Neural Style Transfer Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48be3497bacec44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T13:17:54.136999Z",
     "start_time": "2024-07-23T13:17:54.132553Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "93a75cccb160d4d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T13:17:54.662276Z",
     "start_time": "2024-07-23T13:17:54.637612Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "content = load_image(\"final_project_images/content_images/\" + content_filename, image_size)\n",
    "style = load_image(\"final_project_images/style_images/\" + style_filename, image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b3aaf332a93b078f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T13:17:55.723024Z",
     "start_time": "2024-07-23T13:17:55.211898Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Extract features for perceptual loss\n",
    "per_model = VGG16Features().to(device).eval()\n",
    "content_features_per = extract_features(content, per_model, device)\n",
    "style_features_per = extract_features(style, per_model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "99ce056c6bbc3730",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T13:17:55.727408Z",
     "start_time": "2024-07-23T13:17:55.724040Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "generated_image = content.clone().requires_grad_(True).to(device)\n",
    "optimizer = optim.Adam([generated_image], lr=learning_rate, betas=[0.5, 0.999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a637cdff191bd8f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T13:25:48.525247Z",
     "start_time": "2024-07-23T13:17:55.732785Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: 24542892.0\n",
      "step 100: 18117408.0\n",
      "step 200: 14221697.0\n",
      "step 300: 11561120.0\n",
      "step 400: 9514058.0\n",
      "step 500: 7936760.5\n",
      "step 600: 6749280.0\n",
      "step 700: 5852328.5\n",
      "step 800: 5175324.0\n",
      "step 900: 4658360.5\n",
      "step 1000: 4248566.0\n",
      "step 1100: 3919479.75\n",
      "step 1200: 3650533.0\n",
      "step 1300: 3423652.75\n",
      "step 1400: 3228614.5\n",
      "step 1500: 3057099.75\n",
      "step 1600: 2903657.75\n",
      "step 1700: 2765393.75\n",
      "step 1800: 2641762.75\n",
      "step 1900: 2529119.5\n",
      "step 2000: 2425622.5\n",
      "step 2100: 2330167.75\n",
      "step 2200: 2241648.75\n",
      "step 2300: 2159142.75\n",
      "step 2400: 2082585.125\n",
      "step 2500: 2010579.625\n",
      "step 2600: 1942409.0\n",
      "step 2700: 1878373.75\n",
      "step 2800: 1817628.125\n",
      "step 2900: 1759753.625\n",
      "step 3000: 1704485.25\n",
      "step 3100: 1651778.75\n",
      "step 3200: 1600878.875\n",
      "step 3300: 1552185.5\n",
      "step 3400: 1505932.0\n",
      "step 3500: 1461625.5\n",
      "step 3600: 1419488.625\n",
      "step 3700: 1379239.375\n",
      "step 3800: 1340638.5\n",
      "step 3900: 1303571.25\n",
      "step 4000: 1267408.375\n",
      "saving loss values\n"
     ]
    }
   ],
   "source": [
    "loss_values = []\n",
    "step_list = []\n",
    "for step in range(total_steps):\n",
    "    generated_features = ns_model(generated_image)\n",
    "    content_features = ns_model(content)\n",
    "    style_features = ns_model(style)\n",
    "    \n",
    "    style_loss = 0\n",
    "    content_loss = 0\n",
    "    \n",
    "    for gen_feature, content_feature, style_feature in zip(generated_features,content_features, style_features):\n",
    "        batch_size, channel, height, width = gen_feature.shape\n",
    "        \n",
    "    \n",
    "        # Compute Gram Matrix\n",
    "        G_gen = gen_feature.view(channel, height*width).mm(gen_feature.view(channel, height*width).t())\n",
    "        G_style = style_feature.view(channel, height*width).mm(style_feature.view(channel, height*width).t())\n",
    "        \n",
    "        # Compute Loss\n",
    "        if loss_method == 'Base':\n",
    "            content_loss += torch.mean((gen_feature - content_feature)**2)\n",
    "            style_loss += torch.mean((G_gen - G_style)**2)\n",
    "        elif loss_method == 'Perceptual':\n",
    "            gen_features_per = per_model(generated_image)\n",
    "            content_loss += 100 * torch.mean((gen_features_per - content_features_per) ** 2)\n",
    "            style_loss += 100 * torch.mean((gen_features_per - style_features_per) ** 2)\n",
    "        elif loss_method == 'Wasserstein':\n",
    "            content_loss += torch.mean(generated_image) - torch.mean(content)\n",
    "            style_loss += torch.mean(generated_image) - torch.mean(style)\n",
    "        elif loss_method == 'total_variation':\n",
    "            content_loss += torch.mean(torch.abs(generated_image[:, :, :, 1:] - generated_image[:, :, :, :-1])) + torch.mean(torch.abs(generated_image[:, :, 1:, :] - generated_image[:, :, :-1, :]))\n",
    "            style_loss += torch.mean(torch.abs(generated_image[:, :, :, 1:] - generated_image[:, :, :, :-1])) + torch.mean(torch.abs(generated_image[:, :, 1:, :] - generated_image[:, :, :-1, :]))\n",
    "            \n",
    "       \n",
    "        \n",
    "    total_loss = alpha*content_loss + beta*style_loss\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step %  save_steps == 0:\n",
    "        loss_value = total_loss.item()\n",
    "        loss_values.append(loss_value)\n",
    "        step_list.append(step)\n",
    "        print(f'step {step}: {loss_value}')\n",
    "        image_name = \"final_project_gen_images/\" + image_save_folder + \"/\" + str(step) + \".png\"\n",
    "        save_image(generated_image, image_name)\n",
    "        \n",
    "# Save loss values\n",
    "print('saving loss values')\n",
    "df = pd.DataFrame(list(zip(step_list, loss_values)), columns =['Step', 'Loss'])\n",
    "df.to_csv(\"final_project_gen_images/\" + image_save_folder + \"/loss_values.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c11a0691c47c0aa",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
